### 3. 하드디스크관리, 사용자별 공간 할당
- 하드디스크 추가하기
    - IDE장치와 SCSI장치 구성
        - 메인보드의 IDE0, IDE1슬롯에는 각각 2개의 IDE장치를 장착 가능. 총 4개
        - 일반적으로 PC에서 쓰는 하드디스크나 CD/DVD가 IDE임
        - 서버용은 SCSI
        - 그래서 하드디스크를 추가하려면 IDE에 장치를 장착해야 한다. (보통 1:0 에는 DVD)
        - 나머지 IDE를 SCSI로 변경 가능하나 굳이 필요 없다.
        - VMWare는 SCSI 슬롯을 4개 지원한다. 0번은 0:0 ~ 0:15까지 15개(0:7 제외)의 하드디스크를 장착할 수 있다. 총 4개의 슬롯이므로 60개까지 가능
    - 하드디스크 추가
        - VMWare에서 Server를 클릭 후 Edit virtual machine settings 클릭
        - Hard Disk(SCSI)의 Advanced를 클릭하면 SCSI장치를 확인할 수 있다.
        - 리눅스에서는 처음 장착된 SCSI 하드의 이름을 /dev/sda라고 부른다. 추가로 하드가 장착되면 /dev/sdb, /dev/sdc 등등이 된다.
        - 그리고 각 장치마다 파티션을 나눌 수 있으면 이 파티션은 순차적으로 1, 2, 3, 4로 붙어서 /dev/sda1, sda2.. 등으로 불리우게 된다.
        - 기존에 SCSI를 하나 추가했었고, 이를 /와 swap으로 파티션을 나누었었다. 따라서, swap은 /dev/sda1, /는 /dev/sda2
        - 여기에 SCSI를 하나 더 추가한다. : 추가하면 /dev/sdb가 되고 파티션을 나누지 않는 다면 1개이므로 /dev/sdb1이 된다.
    - 실습
        - edit에서 add를 누르고 hard disk를 scsi로 추가하면 된다.
        - 되도록 이름을 지정할 때는 이름만으로 알 수 있도록 명시하는 것이 좋다.
        - 추가한 후 advanced를 확인하면 scsi 0:1로 추가되어 있을 것이다.
        - 부팅해보면 이제 우측 상단에 하드디스크 마크가 2개 있을 것.
    - 파티션
        - 하드디스크는 장착하면 단순 기계일 뿐이다.
        - 하드디스크를 사용하기 위해선 우선 파티션을 설정해야 한다.
        - 통째로 하나의 파티션으로 사용시에는 1개로 전체를 설정하는데, 2개로 나누려면 2개의 파티션을 설정하면 된다.
        - 파티션은 Primary partition과 Extended로 나뉘는데 1개의 하드는 4개의 Primary파티션으로 나눌 수 있다. 5개로 원한다면 3개까지 Primary로 하고 Extended를 설정한 다음 그 Extended를 2개의 Logical partition으로 나눈다.
    - 파티션 실습
        - 터미널을 켠다
        - fdisk /dev/sdb
        - Command : n (새로운 파티션 분할)
        - Select : p (primary)
        - Partition number : 1 (파티션 개수)
        - First Sector : enter (시작섹터 번호. 1개의 파티션만 나누는 경우 그냥 enter)
        - Last Sector : enter (마지막 섹터 번호)
        - Command : p (설정내용 확인)
        - Command : w (저장)
        - 할당된 파티션 장치의 이름은 /dev/sdb1이 된다.
        - mkfs -t /dev/sdb1 또는 mkfs.ext4 /dev/sdb1 : 파일시스템을 ext4형식으로 생성
        - 만약 새로운 폴더를 /mydata와 같이 만들고 그 안에 아무거나 넣게 되면 해당 파일은 /dev/sda1즉, root partition에 속하게 된다.
        - 따라서 /dev/sdb1에 파일을 저장하고 싶은 경우 우선 해당 하드디스크에 디렉토리를 마운트 시켜야 한다.
        - mount /dev/sdb1 /mydata 와 같이 진행한다.
        - 만약 umount하더라도 해당 파일이 없어지는 것은 아니고 그대로 저장은 되어 있다.
        - 만약 컴퓨터를 켤 때, /dev/sdb1이 항상 /mydata에 mount되어 있기를 바란다면, 아래와 같이 설정한다.
        - vi /etc/fstab
        - /dev/sdb1     /mydata     ext4    defaults    1 2
        - 이 /etc/fstab은 리눅스가 부팅시마다 자동으로 읽는 중요 파일이다.
        - 이 파일에는 마운트 정보가 수록되어 있으며, 글자가 틀리면 아예 부팅이 불가할 수 있다.
        - 6개의 필드는 각각 장치 이름, 마운트될 디렉터리, 파일 시스템, 속성, dump사용여부, 파일 시스템 체크 여부를 의미한다.
        - 파일시스템과 속성을 defaults로 하면 읽기/쓰기/실행 등의 작업이 대부분 가능하다. dump사용여부를 1로 하면 리눅스 dump명령어로 백업이 가능.
        - 파일시스템 체크 여부를 1 또는 2로 하면 부팅 시에 이 파티션을 체크하며, 1을 먼저 체크한다. 0이면 생략함(부팅 속도 상승 가능)
        - reboot하고 ls -l /mydata를 해보면 sdb1에 저장한 파일이 보일 것이다.
- 여러 개의 하드디스크를 하나처럼 사용하기
    - RAID
        - 서버 컴퓨터의 저장 장치 대부분은 하드웨어 RAID 또는 소프트웨어 RAID 방식을 사용
        - RAID : Redundant Array of Inexpensive/Independent Disks로 여러 개의 하드디스크를 하나의 하드처럼 사용하는 방식이다. 비용 절감하며 더 신뢰성도 높이고 성능까지 향상 가능
        - 종류는 크게 하드웨어 RAID, 소프트웨어 RAID로 나뉜다.
        - 하드웨어 RAID
            - 하드웨어 제조업체에서 여러 개의 하드디스크를 연결한 장비를 만들어서 그 자체를 공급하는 것
            - 안정적이고 각 제조업체의 기술 지원을 받아 많이 선호됨
            - 대개 하드웨어 RAID는 고가는 SA-SCSI, 중저가는 SATA를 사용해 만든다.
        - 소프트웨어 RAID
            - 고가인 하드웨어의 대안으로 하드디스크만 여러 개 있으면 운영체제에서 지원하는 방식으로 RAID를 구성하는 방법을 의미함
            - 신뢰성이나 속도가 떨어지지만, 저렴하며 좀 더 안전하ㅔㄱ 데이터를 저장 가능하다.
        - RAID 레벨
            - 기본적으로 구성 방식에 따라, Linear RAID, RAID 0 ~ 5 까지 7개로 구분된다.
            - 실무에서는 ㅈ로 Linear, 0, 1, 5와 5의 변형인 6, 1+0(1과 0의 혼합)을 주로 사용한다.
            - 단순 볼륨 : 하드디스크 하나를 하나의 볼륨으로 사용. RAID방식에는 사용 x. 이미 해본 방식
            - Linear RAID, RAID 0 : 최소 2개의 하드디스크 필요. 2개 이상의 하드를 1개의 볼륨으로 사용. 저장방식에 차이. Linear는 앞의 하드가 다 채워지면 뒤를 사용. RAID 0 은 모든 하드를 사용해 저장.
                - 이와 같이 하드디스크 여러 개에 동시에 저장하는 방식이 stripping인데, 이 방식이 Linear보다 더 빠르다. 그래서 RAID 0 방식이 가장 빠름. 공간 효율도 더 좋다.
                - 문제는 여러 개에 동시에 저장하다 보니 하나가 망가지면 사실상 모든 데이터를 잃는 것과 같다는 것이다.
                - 그래서, RAID 0을 쓰는 것은 빠른 성능이 필요하되, 전부 잃어도 큰 문제는 없는 자료를 저장시에 적합하다.
                - 반면에 Linear는 각각 저장하기 때문에 총공간에 대한 효율이 가장 좋다.
            - RAID 1 : 미러링방식이다. 똑같은 데이터의 거울을 만들어 놓는다는 것. 그래서, 같은 내용을 2번 저장. 총 용량의 절반만 사용.
                - 장점은 하나가 고장나도 데이터 손상이 없다는 것. 이것이 결함허용
                - 단점은 실제 계획보다 2배 큰 용량이 필요하다는 것..
                - 즉, 비용을 감당할 만큼 중요한 데이터의 경우에 사용함.
            - RAID 5 : 1처럼 데이터 안정성을 가지면서도, 0처럼 공간 효율성도 좋은 방식을 어느정도 포용한 것. 최소 3개 이상의 하드가 있어야 구성됨. 대개는 5개 이상으로 함.
                - 하드에 이상이 발생시 패리티를 이용해 데이터 복구.
                - 짝수패리티를 이용하는 경우 데이터를 저장할 때 패리티 공간을 하나 남겨둔 다음, 전체의 덧셈이 짝수가 되도록 유추할 수 있는 방식을 의미함.
                - 이에 따라 결함도 허용되고, 공간 효율도 괜찮음. 패리티로 사용하는 부분을 제외하고는 전부 사용
            - RAID 6 : 허나, RAID 5도 2개가 동시에 고장나면?, 노답이다. 그래서 RAID6는 2개의 패리티를 쓴다 ㅋㅋㅋㅋ
                - 이에 따라, 공간 효율은 좀 낮더라도 신뢰도는 더욱 높아진다.
            - 이외 : 1+0방식의 경우, 1의 미러링으로 구성한 데이터를 다시 0으로 구성하는 것을 의미.
    - 실습
        - 9개의 하드디스크를 준비하자.
        - 알아서 9개를 만든다.(1만 2GB, 나머지는 1GB)
        - 각각을 partition을 나눈다.
        - ls -l /dev/sd*로 해보면 확인 
        - 각 파티션 설정 : 아까와 같지만 이번에는 RAID로 설정하기 위해서 파일시스템 유형을 직접 선택한다. Last Sector이후에 Command : t로 하고 fd 입력후 p, w로 저장
            - fd는 linux raid autodetect임.
        - 다 만들었으면 ls -l /dev/sd*로 확인해본다.
        - halt -p로 끄고 해당 내용을 backup하자
        - Linear RAID 구축
            - 우선 선처리작업을 확인한다. fdisk -l /dev/sdb; fdisk -l /dev/sdc : -l을 이용해 파티션 상태를 출력함
            - mdadm명령어로 실제 RAID구축 : `mdadm --create /dev/md9 --level=linear --raid-devices=2 /dev/sdb1 /dev/sdc1` 로 RAID를 생성(md9은 임의로 지정한 이름이다.)
                - --create는 RAID를 md9장치에 생성
                - --level=linear는  Linear RAID를 지정. 0은 RAID 0, 1은 RAID 1
                - --raid-devices=2 /dev/sdb1 /dev/sdc1 은 2개 하드를 사용.
                - mdadm --stop /dev/md9 : RAID장치인 md9를 중지
                - mdadm --run /dev/md9 : 중지된 RAID 가동
                - mdadm --detail /dev/md9 : md9의 상세한 내역 출력
            - `mdadm --detail --scan` 으로 RAID확인
            - `mkfs.ext4 /dev/md9`또는 `mkfs -t ext4 /dev/md9`으로 파일 시스템 생성
            - `mkdir raidLinear`로 마운트할 디렉터리 생성. `mount /dev/md9 /raidLinear`로 마운트
            - `df`명령어로 확인 가능
        - RAID 0 구축
            - 앞과 비슷함. 대신 mdadm명령어 사용시 --level=0이다.
            - /dev/sdd, /dev/sde로 만들어본다.
            - 추가로 자동으로 mount되도록 /etc/fstab에 추가한다. : `/dev/md0 /raid0 ext4 defaults 0 1`
        - RAID 1 구축
            - 0과 같음. level이 다름
        - RAID 5 구축
            - 1과 같음 하지만 하드 최소 3개 이상
            - 경고 메세지는 무시해도 된다.
        - 다 만들었으면 꼭 backup을 해둔다.
    - 고장내고 문제발생 해결 실습 
        - df명령어로 다들 잘 mount되어 있는지 확인한다.
        - halt -p로 끄고 나서 edit에서 3 ,5, 7, 9를 삭제하자. 이는 하드가 고장난것과 같다.
        - 다시 boot하면 우측 상단에 하드디스크들에서 말풍선으로 고장상태를 알려준다.
        - 그러다가 부팅 도중 응급모드로 전환될 것이다.
        - root로 접속하고 ls -l /dev/sd*를 해보면 전부 있지 않은 것을 확인가능하다.
        - df로 확인하면 기존의 Linear, 0, 1, 5가 확인되지 않는다.
        - 결함이 허용되는 RAID1과 RAID5를 가동해보자 : `mdadm --run /dev/md1`으로 1만 가동한다.
        - system-fsck~라는 메시지가 나오면 enter
        - 이 때, df를 해보면 md1이 잘 나오는 것을 확인 가능. mdadm --detail /dev/md1을 확인해보면 sdd1만 작동하는 것을 알 수 있다.
        - 마찬가지로 5도 가동
        - 0과 Linear는 재가동이 불가하다. 1개의 하드디스크로는 아예 작동하지 않기 때문이다.
        - 이제, /etc/fstab에서 0과 Linear는 주석처리하고 재부팅해보자
        - 그러면 md1과 md5만 잘 작동하는 것을 알 수 있다.
    - 복구
        - 새로이 하드디스크를 edit에서 만들고 부팅한다.
        - 다시 x윈도가 나오지만 복구된 것은 아니다.
        - fdisk명령을 통해서 c, e, g, i의 파티션을 설정한다.
        - Linear와 0같은 경우에는 아예 새로 만들어야 한다. stop시키고 mdadm --create를 통해서 다시 설정한다.
        - 1과 5같은 경우는 하드 하나가 빠졌을 뿐 잘 작동하기 때문에 다음과 같이 하드디스크만 추가한다.
        - `mdadm /dev/md1 --add /dev/sdg1`
        - `mdadm /dev/md5 --add /dev/sdi1`
        - detail명령어로 잘 작동하는지 확인하고 /etc/fstab에서 주석을 풀고 재부팅 해보자
        - 이제는 정상적인 부팅이 가능하다.
        - ls 명령어로 raid0를 확인시 당연히 testFile은 보이지 않아야 한다.
        - 만약 보인다고 해도 그것은 완벽한 파일이 아니다.. 따라서 mkfs로 포맷하자.(umount로 마운트를 제거한 후 해야함)
        - 또한, Linear쪽에서 testFile이 보일 수도 있다. 이것은 운이 좋게도, 데이터가 2GB였던 곳에 저장한 상태에서 1GB짜리 후순위 하드를 고장냈기 때문이다. 이를 항상 기대해선 안된다..
- 고급 RAID 레벨
    - RAID 6
        - 6는 패리티를 2개 사용한다고 했다. 따라서 하드가 최소 4개 이상
        - 실무는 당연히 7~8개 이상으로 구성함
        - 구현방법은 5와 비슷함. 다만, 하드가 더 필요할 뿐
    - RAID 0+1
        - 1은 안전성(미러링), 0은 속도(분할 저장)이라고 보면 된다.
    - 구현
        - Server를 초기화하고 8개 하드를 만든다.
        - RAID 6
            - `mdadm --create /dev/md6 --level=6 --raid-devices=4 /dev/sdb1 /dev/sdc1 /dev/sdd1 /dev/sde1`
            - 이후 `mdadm --detail /dev/md6`로 확인
            - `mkfs.ext4 /dev/md6`로 포맷
            - `mkdir /raid6`로 만들고 `mount /dev/md6 /raid6`로 마운트 시킨다.
        - RAID 0+1
            - `mdadm --create /dev/md2 --level=1 --raid-devices=2 /dev/sdf1 /dev/sdg1`
            - `mdadm --create /dev/md3 --level=1 --raid-devices=2 /dev/sdh1 /dev/sdi1`
            - raid1을 두 개 만들고 두 RAID 1장치를 RAID 0으로 묶는다.
            - `mdadm --create /dev/md10 --level=0 --raid-devices=2 /dev/md2 /dev/md3`
            - `mkfs.ext4 /dev/md10`으로 포맷
            - `mkdir /raid10`으로 만들고 `mount /dev/md10 /raid10`으로 마운트 시킨다.
        - 아무 파일이나 복사해서 raid6 과 raid10에 넣자
        - 시스템을 종료하고 문제발생 테스트를 진행한다.
    - 문제발생 테스트
        - 0:2, 0:4, 0:6, 0:8을 고장내자
        - 응급모드로 부팅되면 root로 접속한다.
        - 먼저 `mdadm --run /dev/md6`, `mdadm --run /dev/md2`, `mdadm --run /dev/md3`로 차례대로 작동시켜보자
        - 각 RAID의 하드가 2개씩 모두 고장 났지만 데이터는 안전하다.
        - 장치 구성을 확인해보면 md6의 경우 2개만 작동중일 것이다.
        - 1+0의 경우도 한개씩만 각각 작동한다.
        - 시스템을 종료한다.
        - 이제 다시 하드를 추가하고 부팅하여 파티션을 나누고 하드만 add하면 된다.
- LVM
    - 개념
        - Logical Volume Manager이다. 논리 하드디스크 관리자라고 보면 된다.
        - Linear RAID와 비슷해 보이나, 더 많은 기능을 갖는다.
        - 그래서 CentOS는 기본적으로 설치 시에 LVM으로 하드를 분할해 설치한다.
    - 용도
        - 여러 하드를 합쳐서 한 개의 파티션으로 구성한 후에, 다시 필요에 따라 나눈다.
        - 또는 한 개의 하드로 LVM을 구성하고 다시 파티션을 구분할 수 있다.
        - 예로 2TB의 하드를 2개 합치고 1,3으로 나눌 수도 있다.
    - 용어
        - 물리볼륨 : /dev/sda1, /dev/sdb1 과 같은 파티션을 의미
        - 볼륨그룹 : 물리볼륨을 합쳐서 1개의 물리 그룹으로 만든 것
        - 논리볼륨 : 불륨 그룹을 1개 이상으로 나눈 것으로 논리적 그룹이라고도 한다.
    - 구현
        - 우선 Server를 초기화하고 하드 두개를 만들자. 2G, 3G로 만든다.
        - 선처리작업을 수행한다.
            - SCSI하드에 파티션을 할당한다.  LVM의 경우 별도로 파일시스템 유형을 8e(Linux LVM)으로 지정해야 한다.
            - 이전의 파티션 작업과 같다. fdisk로 지정하되, 파일시스템만 8e로
            - pvcreate /dev/sdb1, pvcreate /dev/sdc1으로 물리적인 볼륨을 만든다.
            - 볼륨 그룹 만들기 : `vgcreate myVG /dev/sdb1 /dev/sdc1`을 입력해 특정 이름으로 그룹을 만든다.
            - `vgdisplay`로 볼륨 그룹이 만들어졌는지 확인하자
            - 이제는 /dev/myVG를 하나의 하드디스크로 생각하고 작업할 수 있다.
            - 파티션 생성 : `lvcreate`명령을 쓴다.
            - `lvcreate --size 1G --name myLG1 myVG`
            - `lvcreate --size 2G --name myLG2 myVG`
            - `lvcreate --extents 100%FREE --name myLG3 myVG`
            - `ls -l /dev/myVG`로 확인
            - 확인했겠으나, myLG!은 실제로는 dm-0이라는 파일에 링크되어 있다. 
            - 이제 포맷한다. mkfs를 이용
            - `mkfs.ext4 /dev/myVG/myLG1~3`
            - 이제 mkdir로 각 디렉토리를 만들고 각각을 mount시키자.
            - /etc/fstab에 각각의 myLG를 자동 mount하도록 설정하고 재부팅하면 된다.
- RAID 1에 CentOS설치
    - 가상머신을 새로 만든다.
    - 파티션을 분할 할 때, swap과 root를 RAID 1레벨로 지정하여 만들면 된다.
    - 나머지는 같음. 교재 참조
- 사용자별 공간 할당
    - 리눅스는 여러 사용자가 동시 접속해 사용한다.
    - 만약, A라는 사용자가 시스템 사용시에 /파일 시스템에 고의든 실수든 큰 파일들을 계속 복사했다고 가정하자. 하드가 꽉 차면 시스템 전체가 가동되지 않는다!
    - 이런 것을 방지하기 위해 각 사용자별로 사용가능한 용량을 제한해야 한다. 즉, 사용자가 적정 용량 이상을 사용치 못하게 하여 할당된 양만큼의 공간만 사용하게 한다면 문제가 발생ㅎ아지 않을 것이다.
    - 쿼터
        - 쿼터는 파일 시스템마다 사용자나 그룹이 생성할 수 있는 파일의용량과 개수를 제한하는 것을 의미
        - 구현
            - Server초기화
            - 새로운 하드디스크를 장착시키고 부팅하여 root접속
            - /dev/sdb의 파티션을 생성하고 포맷한 다음, /userHome이라는 디렉토리에 mount
            - 그리고 /etc/fstab에 추가
            - 실습을 위해 유저 2을 만든다. `useradd -d /userHome/john john`, bahn도 만들자
            - passwd는 hostname과 같이 하자
            - vi /etc/fstab으로 하고 defaults,usrjquota=aquota.user,jqfmt-vfsv0으로 변경
            - `mount --options remount /userHome`을 통해 재마운트한다.
            - mount로 확인해보면 /dev/sdb1 디렉터리가 쿼터용으로 마운트된 것을 확인할 수 있다.
        - 쿼터를 사용하기 위해서 쿼터 DB를 생성한다.
        - 구현
            - cd /userHome
            - `quotaoff -avug` : 일단 쿼터를 끈다
            - `quotacheck -augmn` : 파일 시스템의 쿼터 관련 체크
            - `rm -rf aquota.*` : 생성된 쿼터 파일 삭제
            - `quotacheck -augmn` : 다시 파일 시스템의 쿼터 관련 체크
            - `touch aquota.user aquota.group` : 쿼터 관련 파일 생성
            - `chmod 600 aquota.*` : 권한 변경(소유자만 가능토록)
            - `quotacheck -augmn` : 쿼터 체크
            - `quotaon -avug` : 쿼터 시작
        - 명령어
            - quotacheck는 하드디스크를 스캔해 여러 가지 부분을 체크하는 기능
            - quotaon/off는 설정된 쿼터를 끄거나 켠다.
            - 옵션
                - -a : 모든 파일 시스템 체크
                - -u : 모든 사용자 쿼터 관련 체크
                - -g : 그룹 쿼터 관련 체크
                - -m : 재마운트를 생략
                - -n : 첫 번째 검색된 것을 사용함
                - -p : 처리 결과를 출력
                - -v : 파일 시스템의 상태를 보여줌
    - 각 사용자별 공간 할당
        - 공간을 각각 john과 bahn에게 10MB씩 할당
        - `edquota -u john` 을 입력시 각 사용자별 또는 그룹별 할당량을 편집 가능
        - 사용법은 vi에디터와 동일
        - 각 의미
            - Filesystem : 사용자별 쿼터를 할당하는 파일 시스템 앞에서는 /etc/fstab에서 했었다.
            - blocks, soft, hard : 현재 사용자가 사용하는 블록과 소프트 사용 한도, 하드 사용 한도를 의미
                - blocks가 28이면 현재 john이 28kb를 사용. soft와 hard가 0이면 사용한도 제한이 없단것.
            - inodes, soft, hard : inode의 개수. soft, hard는 한도
        - blocks부분에 soft 10MB, hard 15MB로 제한하고 저장한다.
        - 칸을 맞출 필요는 없다.  이제는 john사용자가 사용가능한 용량이 설정대로 제한되어 있는지 확인
        - `su - john` : john으로 접속
        - `cp /boot/vmlinuz-3* test1` : 약 4.7MB
        - `cp test1 test2` : 약 9.4MB
        - `cp test1 test3` : 약 14.1MB(소프트한도 초과)
        - `cp test1 test4` : 약 18.8MB(하드 한도 초과. 더 사용 불가.)
        - 하드 한도를 초과하여 test4파일은 하드 한도까지만 파일이 생성된다. 따라서 test4는 정상적인 파일이 아님
        - john사용자는 `quota`명령어로 자신에게 할당된 하드디스크 공간을 확인가능
        - 잘 살펴보면 john에게 할당된 것인 limit의 내용이 아닌 quota의 내용
        - 그래서 quota를 넘는 부분 즉, limit - quota는 grace만큼만 유효함. 그 기간동안 공간을 정리해야 한다.
        - root로 이동하고 `repquota /userHome`으로 사용자별 현재 사용량을 확인하자.
        - bahn에게 john하고 같은 수준으로 할당하고 싶다면 `edquota -p john bahn`으로 하면 된다.
        - 참고로 소프트 한도 초과에 대한 유예기간의 기본값은 7days이다. 이를 조정하려는 경우 `edquota -t`로 변경할 수 있다.